All seeds set to 42 for random, numpy, torch
2025-05-14 02:25:50,318 - PyTorch version 2.4.1+cu121 available.
2025-05-14 02:25:50,319 - Duckdb version 1.1.3 available.
2025-05-14 02:25:50,950 - Device <cuda:0> is selected
2025-05-14 02:25:50,950 - Device <cuda:0> is selected
2025-05-14 02:25:50,950 - Device <cuda:0> is selected
2025-05-14 02:25:52,267 - Device <cuda:0> is selected

=== Starting task: img_cls_cifar ===
Files already downloaded and verified
Files already downloaded and verified

-- PEFT combo #0: ('low-rank', 'quant-dynamic', 'pruning', 'quant-static') --
2025-05-14 02:25:53,509 - Device <cuda:0> is selected
2025-05-14 02:25:53,911 - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2025-05-14 02:25:53,945 - State start
2025-05-14 02:25:53,955 -   Scheduler at: inproc://10.32.15.89/231603/1
2025-05-14 02:25:53,956 -   dashboard at:  http://10.32.15.89:44601/status
2025-05-14 02:25:53,966 -       Start worker at: inproc://10.32.15.89/231603/4
2025-05-14 02:25:53,966 -          Listening to:          inproc10.32.15.89
2025-05-14 02:25:53,966 -           Worker name:                          0
2025-05-14 02:25:53,966 -          dashboard at:          10.32.15.89:41079
2025-05-14 02:25:53,966 - Waiting to connect to: inproc://10.32.15.89/231603/1
2025-05-14 02:25:53,966 - -------------------------------------------------
2025-05-14 02:25:53,966 -               Threads:                          4
2025-05-14 02:25:53,966 -                Memory:                  62.97 GiB
2025-05-14 02:25:53,966 -       Local Directory: /tmp/dask-scratch-space/worker-p8ckal1j
2025-05-14 02:25:53,966 - -------------------------------------------------
2025-05-14 02:25:53,969 - Register worker <WorkerState 'inproc://10.32.15.89/231603/4', name: 0, status: init, memory: 0, processing: 0>
2025-05-14 02:25:53,970 - Starting worker compute stream, inproc://10.32.15.89/231603/4
2025-05-14 02:25:53,970 - Starting established connection to inproc://10.32.15.89/231603/5
2025-05-14 02:25:53,970 -         Registered to: inproc://10.32.15.89/231603/1
2025-05-14 02:25:53,970 - -------------------------------------------------
2025-05-14 02:25:53,970 - Starting established connection to inproc://10.32.15.89/231603/1
2025-05-14 02:25:53,973 - Receive client connection: Client-9d66859c-3051-11f0-88b3-a036bccd403d
2025-05-14 02:25:53,973 - Starting established connection to inproc://10.32.15.89/231603/6
2025-05-14 02:25:54,135 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:25:54,144 - ApiComposer - Pipeline composition started.
All seeds set to 42 for random, numpy, torch
2025-05-14 02:29:31,346 - PyTorch version 2.4.1+cu121 available.
2025-05-14 02:29:31,347 - Duckdb version 1.1.3 available.
2025-05-14 02:29:31,968 - Device <cuda:0> is selected
2025-05-14 02:29:31,969 - Device <cuda:0> is selected
2025-05-14 02:29:31,969 - Device <cuda:0> is selected
2025-05-14 02:29:33,307 - Device <cuda:0> is selected

=== Starting task: img_cls_cifar ===
Files already downloaded and verified
Files already downloaded and verified

-- PEFT combo #0: ('low-rank', 'quant-dynamic', 'pruning', 'quant-static') --
2025-05-14 02:29:34,528 - Device <cuda:0> is selected
2025-05-14 02:29:34,914 - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2025-05-14 02:29:34,946 - State start
2025-05-14 02:29:34,956 -   Scheduler at: inproc://10.32.15.89/236034/1
2025-05-14 02:29:34,957 -   dashboard at:  http://10.32.15.89:38069/status
2025-05-14 02:29:34,966 -       Start worker at: inproc://10.32.15.89/236034/4
2025-05-14 02:29:34,966 -          Listening to:          inproc10.32.15.89
2025-05-14 02:29:34,966 -           Worker name:                          0
2025-05-14 02:29:34,967 -          dashboard at:          10.32.15.89:34599
2025-05-14 02:29:34,967 - Waiting to connect to: inproc://10.32.15.89/236034/1
2025-05-14 02:29:34,967 - -------------------------------------------------
2025-05-14 02:29:34,967 -               Threads:                          4
2025-05-14 02:29:34,967 -                Memory:                  62.97 GiB
2025-05-14 02:29:34,967 -       Local Directory: /tmp/dask-scratch-space/worker-77y627hf
2025-05-14 02:29:34,967 - -------------------------------------------------
2025-05-14 02:29:34,970 - Register worker <WorkerState 'inproc://10.32.15.89/236034/4', name: 0, status: init, memory: 0, processing: 0>
2025-05-14 02:29:34,970 - Starting worker compute stream, inproc://10.32.15.89/236034/4
2025-05-14 02:29:34,970 - Starting established connection to inproc://10.32.15.89/236034/5
2025-05-14 02:29:34,970 -         Registered to: inproc://10.32.15.89/236034/1
2025-05-14 02:29:34,971 - -------------------------------------------------
2025-05-14 02:29:34,971 - Starting established connection to inproc://10.32.15.89/236034/1
2025-05-14 02:29:34,973 - Receive client connection: Client-212089a8-3052-11f0-9a02-a036bccd403d
2025-05-14 02:29:34,973 - Starting established connection to inproc://10.32.15.89/236034/6
2025-05-14 02:29:35,102 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:29:35,111 - ApiComposer - Pipeline composition started.
2025-05-14 02:31:03,185 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:32:33,103 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:32:33,106 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 02:32:33,356 - ApiComposer - Model generation finished
2025-05-14 02:33:51,313 - FEDOT logger - Final pipeline was fitted
2025-05-14 02:33:51,314 - FEDOT logger - Final pipeline: {'depth': 1, 'length': 1, 'nodes': [low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 02:34:01,943 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:34:01,950 - ApiComposer - Pipeline composition started.
2025-05-14 02:34:17,188 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:34:32,744 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:34:32,750 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 02:34:33,007 - ApiComposer - Model generation finished
2025-05-14 02:34:33,575 - FEDOT logger - Final pipeline was fitted
2025-05-14 02:34:33,576 - FEDOT logger - Final pipeline: {'depth': 1, 'length': 1, 'nodes': [quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 02:34:44,735 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:34:44,742 - ApiComposer - Pipeline composition started.
2025-05-14 02:35:05,069 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 02:39:01,800 - Scheduler was unaware of this worker 'inproc://10.32.15.89/236034/12'. Shutting down.
2025-05-14 02:39:31,902 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 02:40:14,682 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 02:41:44,447 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 02:44:06,600 - MetricsObjective - Objective evaluation error for graph {'depth': 1, 'length': 1, 'nodes': [pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 2.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 4.07 GiB memory in use. Process 1391864 has 40.58 GiB memory in use. Of the allocated memory 3.40 GiB is allocated by PyTorch, and 162.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 02:44:07,719 - MetricsObjective - Objective evaluation error for graph {'depth': 1, 'length': 1, 'nodes': [pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 2.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 4.07 GiB memory in use. Process 1391864 has 40.58 GiB memory in use. Of the allocated memory 3.39 GiB is allocated by PyTorch, and 164.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 02:47:45,659 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
Failed step pruning for ResNet18 on CIFAR10
2025-05-14 02:47:45,912 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:47:45,920 - ApiComposer - Pipeline composition started.
2025-05-14 02:47:54,056 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [quantization_model, quantization_model]} on metric accuracy: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 12.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 6.79 GiB memory in use. Process 1395845 has 37.85 GiB memory in use. Of the allocated memory 6.00 GiB is allocated by PyTorch, and 284.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 02:47:56,230 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [quantization_model, quantization_model]} on metric accuracy: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 12.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 6.79 GiB memory in use. Process 1395845 has 37.85 GiB memory in use. Of the allocated memory 6.00 GiB is allocated by PyTorch, and 281.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 02:47:56,604 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
Failed step quant-static for ResNet18 on CIFAR10

-- PEFT combo #1: ('low-rank', 'quant-dynamic', 'pruning', 'quant-qat') --
2025-05-14 02:47:56,884 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:47:56,892 - ApiComposer - Pipeline composition started.
2025-05-14 02:50:46,738 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:53:15,846 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 02:53:26,809 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 02:53:54,153 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:53:54,178 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 02:53:54,449 - ApiComposer - Model generation finished
2025-05-14 02:56:17,363 - FEDOT logger - Final pipeline was fitted
2025-05-14 02:56:17,364 - FEDOT logger - Final pipeline: {'depth': 2, 'length': 2, 'nodes': [low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 02:56:30,061 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:56:30,073 - ApiComposer - Pipeline composition started.
2025-05-14 02:56:59,833 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:57:30,172 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 02:57:30,177 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 02:57:30,459 - ApiComposer - Model generation finished
2025-05-14 02:57:36,351 - FEDOT logger - Final pipeline was fitted
2025-05-14 02:57:36,352 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 3, 'nodes': [quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 02:57:53,622 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 02:57:53,631 - ApiComposer - Pipeline composition started.
2025-05-14 03:00:30,243 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [pruning_model, pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 2.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 2.68 GiB memory in use. Process 1417766 has 41.97 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 55.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 03:00:30,969 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [pruning_model, pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 2.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 2.68 GiB memory in use. Process 1417766 has 41.97 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 57.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 03:02:00,005 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:02:51,873 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [pruning_model, pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 2.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 2.53 GiB memory in use. Process 1417766 has 42.12 GiB memory in use. Of the allocated memory 1.94 GiB is allocated by PyTorch, and 81.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 03:02:52,412 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [pruning_model, pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 2.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 2.53 GiB memory in use. Process 1417766 has 42.12 GiB memory in use. Of the allocated memory 1.94 GiB is allocated by PyTorch, and 80.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 03:03:23,552 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:09:37,932 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [pruning_model, pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 20.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 2.52 GiB memory in use. Process 1427953 has 42.12 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 73.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 03:09:38,534 - MetricsObjective - Objective evaluation error for graph {'depth': 2, 'length': 2, 'nodes': [pruning_model, pruning_model]} on metric accuracy: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 20.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 2.52 GiB memory in use. Process 1427953 has 42.12 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 72.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 03:09:38,912 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
Traceback (most recent call last):
  File "experiments.py", line 243, in <module>
    compressor.fit(train_data)
  File "/ptls-experiments/FedCore/fedcore/api/main.py", line 163, in fit
    self.fedcore_model = Maybe.insert(self._process_input_data(input_data)). \
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/monad.py", line 152, in then
    result = self.map(function)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/maybe.py", line 65, in map
    return self.__class__(function(self.value), True) # pytype: disable=not-callable
  File "/ptls-experiments/FedCore/fedcore/api/main.py", line 158, in fit_function
    fitted_solver = self.manager.solver.fit(train_data)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/api/main.py", line 185, in fit
    self.current_pipeline, self.best_models, self.history = self.api_composer.obtain_model(self.train_data)
  File "/ptls-experiments/FedCore/fedcore/repository/fedcore_impl/abstract.py", line 111, in obtain_model_fedcore
    best_pipeline, best_pipeline_candidates, gp_composer = self.compose_pipeline(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/api/api_utils/api_composer.py", line 142, in compose_pipeline
    best_pipelines = gp_composer.compose_pipeline(data=train_data)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/composer/gp_composer/gp_composer.py", line 67, in compose_pipeline
    opt_result = self.optimizer.optimise(objective_function)
  File "/ptls-experiments/FedCore/fedcore/interfaces/fedcore_optimizer.py", line 252, in optimise
    population_to_eval, evaluator = Either.insert(objective). \
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/monad.py", line 152, in then
    result = self.map(function)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/either.py", line 106, in map
    return self.__class__(function(self.value), (None, True))
  File "/ptls-experiments/FedCore/fedcore/interfaces/fedcore_optimizer.py", line 254, in <lambda>
    then(lambda evaluator: self._initial_population(evaluator)).value
  File "/ptls-experiments/FedCore/fedcore/interfaces/fedcore_optimizer.py", line 100, in _initial_population
    self._update_population(next_population=init_population, evaluator=evaluator, label=label)
  File "/ptls-experiments/FedCore/fedcore/interfaces/fedcore_optimizer.py", line 169, in _update_population
    self.generations.append(next_population)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/golem/core/optimisers/archive/generation_keeper.py", line 137, in append
    self.archive.update(population)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/golem/core/optimisers/archive/individuals_containers.py", line 156, in update
    if not dominates_one and hof_member.fitness.dominates(ind.fitness):
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/golem/core/optimisers/fitness/multi_objective_fitness.py", line 90, in dominates
    for self_wvalue, other_wvalue in zip(self.wvalues[selector], other.wvalues[selector]):
AttributeError: 'SingleObjFitness' object has no attribute 'wvalues'
Traceback (most recent call last):
  File "experiments.py", line 243, in <module>
    compressor.fit(train_data)
  File "/ptls-experiments/FedCore/fedcore/api/main.py", line 163, in fit
    self.fedcore_model = Maybe.insert(self._process_input_data(input_data)). \
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/monad.py", line 152, in then
    result = self.map(function)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/maybe.py", line 65, in map
    return self.__class__(function(self.value), True) # pytype: disable=not-callable
  File "/ptls-experiments/FedCore/fedcore/api/main.py", line 158, in fit_function
    fitted_solver = self.manager.solver.fit(train_data)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/api/main.py", line 185, in fit
    self.current_pipeline, self.best_models, self.history = self.api_composer.obtain_model(self.train_data)
  File "/ptls-experiments/FedCore/fedcore/repository/fedcore_impl/abstract.py", line 111, in obtain_model_fedcore
    best_pipeline, best_pipeline_candidates, gp_composer = self.compose_pipeline(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/api/api_utils/api_composer.py", line 142, in compose_pipeline
    best_pipelines = gp_composer.compose_pipeline(data=train_data)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/composer/gp_composer/gp_composer.py", line 67, in compose_pipeline
    opt_result = self.optimizer.optimise(objective_function)
  File "/ptls-experiments/FedCore/fedcore/interfaces/fedcore_optimizer.py", line 251, in optimise
    population_to_eval, evaluator = self._initial_population(evaluator)
  File "/ptls-experiments/FedCore/fedcore/interfaces/fedcore_optimizer.py", line 100, in _initial_population
    self._update_population(next_population=init_population, evaluator=evaluator, label=label)
  File "/ptls-experiments/FedCore/fedcore/interfaces/fedcore_optimizer.py", line 169, in _update_population
    self.generations.append(next_population)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/golem/core/optimisers/archive/generation_keeper.py", line 138, in append
    self._update_improvements(previous_archive_fitness)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/golem/core/optimisers/archive/generation_keeper.py", line 154, in _update_improvements
    if is_metric_worse(previous_worst, current_worst):
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/golem/core/optimisers/fitness/fitness.py", line 170, in is_metric_worse
    return left_value > right_value
TypeError: '>' not supported between instances of 'float' and 'NoneType'
Traceback (most recent call last):
  File "experiments.py", line 243, in <module>
    compressor.fit(train_data)
  File "/ptls-experiments/FedCore/fedcore/api/main.py", line 163, in fit
    self.fedcore_model = Maybe.insert(self._process_input_data(input_data)). \
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/monad.py", line 152, in then
    result = self.map(function)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/maybe.py", line 65, in map
    return self.__class__(function(self.value), True) # pytype: disable=not-callable
  File "/ptls-experiments/FedCore/fedcore/api/main.py", line 158, in fit_function
    fitted_solver = self.manager.solver.fit(train_data)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/api/main.py", line 185, in fit
    self.current_pipeline, self.best_models, self.history = self.api_composer.obtain_model(self.train_data)
  File "/ptls-experiments/FedCore/fedcore/repository/fedcore_impl/abstract.py", line 111, in obtain_model_fedcore
    best_pipeline, best_pipeline_candidates, gp_composer = self.compose_pipeline(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/api/api_utils/api_composer.py", line 142, in compose_pipeline
    best_pipelines = gp_composer.compose_pipeline(data=train_data)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/composer/gp_composer/gp_composer.py", line 67, in compose_pipeline
    opt_result = self.optimizer.optimise(objective_function)
Failed step pruning for ResNet18 on CIFAR10
2025-05-14 03:09:39,166 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:09:39,176 - ApiComposer - Pipeline composition started.
2025-05-14 03:11:34,941 - MetricsObjective - Objective evaluation error for graph {'depth': 4, 'length': 4, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model]} on metric accuracy: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 16.19 MiB is free. Process 1238022 has 642.00 MiB memory in use. Process 1268611 has 904.00 MiB memory in use. Process 1362619 has 620.00 MiB memory in use. Process 1364690 has 620.00 MiB memory in use. Process 1372002 has 12.09 GiB memory in use. Process 1427953 has 32.55 GiB memory in use. Of the allocated memory 11.02 GiB is allocated by PyTorch, and 568.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-14 03:11:48,623 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:14:16,569 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:14:16,578 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 03:14:16,903 - ApiComposer - Model generation finished
2025-05-14 03:15:09,092 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:16:05,799 - FEDOT logger - Final pipeline was fitted
2025-05-14 03:16:05,800 - FEDOT logger - Final pipeline: {'depth': 4, 'length': 4, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #2: ('low-rank', 'quant-static', 'pruning', 'quant-dynamic') --
2025-05-14 03:16:26,910 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:16:26,921 - ApiComposer - Pipeline composition started.
2025-05-14 03:20:23,289 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:21:56,857 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:24:06,454 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:24:06,473 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 03:24:06,778 - ApiComposer - Model generation finished
2025-05-14 03:27:26,360 - FEDOT logger - Final pipeline was fitted
2025-05-14 03:27:26,361 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 3, 'nodes': [low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 03:27:42,452 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:27:42,465 - ApiComposer - Pipeline composition started.
2025-05-14 03:30:13,793 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:32:55,700 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:32:55,709 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 03:32:56,028 - ApiComposer - Model generation finished
2025-05-14 03:33:12,386 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:35:01,776 - FEDOT logger - Final pipeline was fitted
2025-05-14 03:35:01,777 - FEDOT logger - Final pipeline: {'depth': 5, 'length': 5, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 03:35:25,000 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:35:25,010 - ApiComposer - Pipeline composition started.
2025-05-14 03:40:54,934 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:53:34,884 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 04:11:24,944 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 04:11:24,957 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:11:25,256 - ApiComposer - Model generation finished
2025-05-14 04:14:28,406 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:14:28,407 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 3, 'nodes': [pruning_model, pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 04:14:45,820 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:14:45,836 - ApiComposer - Pipeline composition started.
2025-05-14 04:17:13,737 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:19:44,923 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:19:44,931 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:19:45,281 - ApiComposer - Model generation finished
2025-05-14 04:20:15,790 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:21:32,296 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:21:32,297 - FEDOT logger - Final pipeline: {'depth': 6, 'length': 6, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #3: ('low-rank', 'quant-static', 'pruning', 'quant-qat') --
2025-05-14 04:21:57,909 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:21:57,921 - ApiComposer - Pipeline composition started.
2025-05-14 04:27:01,498 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:27:27,859 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:32:04,753 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:32:04,764 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:32:05,072 - ApiComposer - Model generation finished
2025-05-14 04:36:30,106 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:36:30,107 - FEDOT logger - Final pipeline: {'depth': 4, 'length': 4, 'nodes': [low_rank_model, low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 04:36:49,031 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:36:49,046 - ApiComposer - Pipeline composition started.
2025-05-14 04:39:38,671 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:41:48,884 - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/worker.py", line 1237, in heartbeat
    response = await retry_operation(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/core.py", line 1359, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/core.py", line 1118, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/comm/inproc.py", line 215, in read
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-05-14 04:42:18,985 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:42:32,300 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:42:32,310 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:42:32,643 - ApiComposer - Model generation finished
2025-05-14 04:44:31,408 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:44:31,410 - FEDOT logger - Final pipeline: {'depth': 7, 'length': 7, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 04:45:00,674 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:45:00,685 - ApiComposer - Pipeline composition started.
2025-05-14 04:50:30,604 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 05:08:40,799 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 05:32:34,456 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 05:32:34,471 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 05:32:34,793 - ApiComposer - Model generation finished
2025-05-14 05:36:46,029 - FEDOT logger - Final pipeline was fitted
2025-05-14 05:36:46,030 - FEDOT logger - Final pipeline: {'depth': 4, 'length': 4, 'nodes': [pruning_model, pruning_model, pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 05:37:06,681 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 05:37:06,700 - ApiComposer - Pipeline composition started.
2025-05-14 05:41:32,723 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:42:36,617 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 05:45:50,923 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:45:50,931 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 05:45:51,264 - ApiComposer - Model generation finished
2025-05-14 05:49:16,153 - FEDOT logger - Final pipeline was fitted
2025-05-14 05:49:16,154 - FEDOT logger - Final pipeline: {'depth': 8, 'length': 8, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #4: ('low-rank', 'quant-qat', 'pruning', 'quant-dynamic') --
2025-05-14 05:49:46,731 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 05:49:46,743 - ApiComposer - Pipeline composition started.
2025-05-14 05:55:16,672 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 05:56:09,395 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:02:29,726 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:02:29,738 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 06:02:30,064 - ApiComposer - Model generation finished
2025-05-14 06:08:22,309 - FEDOT logger - Final pipeline was fitted
2025-05-14 06:08:22,311 - FEDOT logger - Final pipeline: {'depth': 5, 'length': 5, 'nodes': [low_rank_model, low_rank_model, low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 06:08:44,250 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 06:08:44,269 - ApiComposer - Pipeline composition started.
2025-05-14 06:13:44,097 - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/worker.py", line 1237, in heartbeat
    response = await retry_operation(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/core.py", line 1359, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/core.py", line 1118, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/distributed/comm/inproc.py", line 215, in read
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-05-14 06:14:14,196 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 06:15:24,783 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:22:03,509 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:22:03,521 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 06:22:03,913 - ApiComposer - Model generation finished
2025-05-14 06:27:45,284 - FEDOT logger - Final pipeline was fitted
2025-05-14 06:27:45,285 - FEDOT logger - Final pipeline: {'depth': 9, 'length': 9, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 06:28:22,200 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 06:28:22,213 - ApiComposer - Pipeline composition started.
2025-05-14 06:33:52,137 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 06:57:47,947 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 07:27:39,168 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 07:27:39,188 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 07:27:39,538 - ApiComposer - Model generation finished
2025-05-14 07:32:55,847 - FEDOT logger - Final pipeline was fitted
2025-05-14 07:32:55,848 - FEDOT logger - Final pipeline: {'depth': 5, 'length': 5, 'nodes': [pruning_model, pruning_model, pruning_model, pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'hessian', 'importance_norm': 1, 'pruning_ratio': 0.3214285714285714, 'importance_reduction': 'mean', 'importance_normalize': 'mean', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'hessian', 'importance_norm': 2, 'pruning_ratio': 0.95, 'importance_reduction': 'max', 'importance_normalize': 'gaussian', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 07:33:19,340 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 07:33:19,360 - ApiComposer - Pipeline composition started.
2025-05-14 07:38:49,272 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 07:39:50,013 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:45:49,798 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:45:49,807 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 07:45:50,191 - ApiComposer - Model generation finished
2025-05-14 07:51:14,305 - FEDOT logger - Final pipeline was fitted
2025-05-14 07:51:14,306 - FEDOT logger - Final pipeline: {'depth': 10, 'length': 10, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #5: ('low-rank', 'quant-qat', 'pruning', 'quant-static') --
2025-05-14 07:51:51,462 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 07:51:51,477 - ApiComposer - Pipeline composition started.
2025-05-14 07:57:21,406 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 07:59:31,944 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:07:19,521 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:07:19,534 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 08:07:19,879 - ApiComposer - Model generation finished
2025-05-14 08:14:14,238 - FEDOT logger - Final pipeline was fitted
2025-05-14 08:14:14,239 - FEDOT logger - Final pipeline: {'depth': 6, 'length': 6, 'nodes': [low_rank_model, low_rank_model, low_rank_model, low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 08:14:39,763 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 08:14:39,788 - ApiComposer - Pipeline composition started.
2025-05-14 08:20:09,700 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 08:23:20,279 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:31:44,435 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:31:44,447 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 08:31:44,846 - ApiComposer - Model generation finished
2025-05-14 08:39:08,930 - FEDOT logger - Final pipeline was fitted
2025-05-14 08:39:08,931 - FEDOT logger - Final pipeline: {'depth': 11, 'length': 11, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 08:39:51,666 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 08:39:51,680 - ApiComposer - Pipeline composition started.
2025-05-14 08:45:21,600 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 09:17:03,562 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 09:54:04,492 - FedcoreDispatcher - 5 individuals out of 5 in previous population were evaluated successfully.
2025-05-14 09:54:04,510 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 09:54:04,865 - ApiComposer - Model generation finished
2025-05-14 10:00:49,630 - FEDOT logger - Final pipeline was fitted
2025-05-14 10:00:49,632 - FEDOT logger - Final pipeline: {'depth': 6, 'length': 6, 'nodes': [pruning_model, pruning_model, pruning_model, pruning_model, pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'lamp', 'importance_norm': 0, 'pruning_ratio': 0.95, 'importance_reduction': 'mean', 'importance_normalize': 'mean', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'taylor', 'importance_norm': 2, 'pruning_ratio': 0.2642857142857143, 'importance_reduction': 'sum', 'importance_normalize': 'sum', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 5, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 10:01:17,735 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 10:01:17,758 - ApiComposer - Pipeline composition started.
2025-05-14 10:06:47,669 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 10:09:15,646 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 10:17:10,276 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 10:17:10,288 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 10:17:10,698 - ApiComposer - Model generation finished
