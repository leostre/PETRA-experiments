All seeds set to 42 for random, numpy, torch
2025-05-14 03:35:49,179 - PyTorch version 2.4.1+cu121 available.
2025-05-14 03:35:49,179 - Duckdb version 1.1.3 available.
2025-05-14 03:35:49,811 - Device <cuda:0> is selected
2025-05-14 03:35:49,811 - Device <cuda:0> is selected
2025-05-14 03:35:49,811 - Device <cuda:0> is selected
2025-05-14 03:35:51,168 - Device <cuda:0> is selected

=== Starting task: img_cls_imagenet ===

-- PEFT combo #0: ('low-rank', 'quant-dynamic', 'pruning', 'quant-static') --

>>> PEFT step 1: low-rank
2025-05-14 03:35:51,271 - Device <cuda:0> is selected
Creating Dask Server
2025-05-14 03:35:51,694 - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2025-05-14 03:35:51,726 - State start
2025-05-14 03:35:51,736 -   Scheduler at: inproc://10.32.15.89/289639/1
2025-05-14 03:35:51,737 -   dashboard at:  http://10.32.15.89:43105/status
2025-05-14 03:35:51,745 -       Start worker at: inproc://10.32.15.89/289639/4
2025-05-14 03:35:51,745 -          Listening to:          inproc10.32.15.89
2025-05-14 03:35:51,745 -           Worker name:                          0
2025-05-14 03:35:51,745 -          dashboard at:          10.32.15.89:36803
2025-05-14 03:35:51,745 - Waiting to connect to: inproc://10.32.15.89/289639/1
2025-05-14 03:35:51,746 - -------------------------------------------------
2025-05-14 03:35:51,746 -               Threads:                          4
2025-05-14 03:35:51,746 -                Memory:                  62.97 GiB
2025-05-14 03:35:51,746 -       Local Directory: /tmp/dask-scratch-space/worker-a0xanvxa
2025-05-14 03:35:51,746 - -------------------------------------------------
2025-05-14 03:35:51,749 - Register worker <WorkerState 'inproc://10.32.15.89/289639/4', name: 0, status: init, memory: 0, processing: 0>
2025-05-14 03:35:51,749 - Starting worker compute stream, inproc://10.32.15.89/289639/4
2025-05-14 03:35:51,749 - Starting established connection to inproc://10.32.15.89/289639/5
2025-05-14 03:35:51,750 -         Registered to: inproc://10.32.15.89/289639/1
2025-05-14 03:35:51,750 - -------------------------------------------------
2025-05-14 03:35:51,750 - Starting established connection to inproc://10.32.15.89/289639/1
2025-05-14 03:35:51,752 - Receive client connection: Client-6378dbdc-305b-11f0-ab67-a036bccd403d
2025-05-14 03:35:51,752 - Starting established connection to inproc://10.32.15.89/289639/6
2025-05-14 03:35:51,874 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:35:51,883 - ApiComposer - Pipeline composition started.
Generations:   0%|          | 0/10000 [00:00<?, ?gen/s]Triggered OptimizerGen at 1 epoch.

Batch #:   0%|          | 0/237 [00:00<?, ?it/s][A
Batch #:   0%|          | 1/237 [00:00<02:26,  1.61it/s][A
Batch #:   1%|          | 2/237 [00:00<01:22,  2.85it/s][A
Batch #:   1%|â–         | 3/237 [00:01<01:09,  3.37it/s][A
Batch #:   2%|â–         | 4/237 [00:01<01:04,  3.64it/s][A
Batch #:   2%|â–         | 5/237 [00:01<00:53,  4.37it/s][A
Batch #:   3%|â–Ž         | 6/237 [00:01<00:45,  5.08it/s][A
Batch #:   3%|â–Ž         | 7/237 [00:01<00:41,  5.55it/s][A
Batch #:   3%|â–Ž         | 8/237 [00:01<00:40,  5.65it/s][A
Batch #:   4%|â–         | 9/237 [00:02<00:39,  5.78it/s][A
Batch #:   4%|â–         | 10/237 [00:02<01:02,  3.66it/s][A
Batch #:   5%|â–         | 11/237 [00:02<00:53,  4.23it/s][A
Batch #:   5%|â–Œ         | 12/237 [00:02<00:48,  4.64it/s][A
Batch #:   5%|â–Œ         | 13/237 [00:03<00:44,  4.99it/s][A
Batch #:   6%|â–Œ         | 14/237 [00:03<00:39,  5.60it/s][A
Batch #:   6%|â–‹         | 15/237 [00:03<00:38,  5.75it/s][A
Batch #:   7%|â–‹         | 16/237 [00:03<00:40,  5.49it/s][A
Batch #:   7%|â–‹         | 17/237 [00:03<00:41,  5.30it/s][A
Batch #:   8%|â–Š         | 18/237 [00:03<00:40,  5.44it/s][A
Batch #:   8%|â–Š         | 19/237 [00:04<00:37,  5.85it/s][A
Batch #:   8%|â–Š         | 20/237 [00:04<00:54,  4.00it/s][A
Batch #:   9%|â–‰         | 21/237 [00:04<00:47,  4.55it/s][A
Batch #:   9%|â–‰         | 22/237 [00:04<00:41,  5.19it/s][A
Batch #:  10%|â–‰         | 23/237 [00:04<00:37,  5.75it/s][A
Batch #:  10%|â–ˆ         | 24/237 [00:05<00:35,  5.96it/s][A
Batch #:  11%|â–ˆ         | 25/237 [00:05<00:34,  6.09it/s][A
Batch #:  11%|â–ˆ         | 26/237 [00:05<00:32,  6.55it/s][A
Batch #:  11%|â–ˆâ–        | 27/237 [00:05<00:33,  6.26it/s][A
Batch #:  12%|â–ˆâ–        | 28/237 [00:05<00:32,  6.39it/s][A
Batch #:  12%|â–ˆâ–        | 29/237 [00:05<00:31,  6.64it/s][A
Batch #:  13%|â–ˆâ–Ž        | 30/237 [00:06<00:59,  3.49it/s][A
Batch #:  13%|â–ˆâ–Ž        | 31/237 [00:06<00:51,  4.00it/s][A
Batch #:  14%|â–ˆâ–Ž        | 32/237 [00:06<00:44,  4.60it/s][A
Batch #:  14%|â–ˆâ–        | 33/237 [00:06<00:43,  4.71it/s][A
Batch #:  14%|â–ˆâ–        | 34/237 [00:07<00:38,  5.23it/s][A
Batch #:  15%|â–ˆâ–        | 35/237 [00:07<00:36,  5.49it/s][A
Batch #:  15%|â–ˆâ–Œ        | 36/237 [00:07<00:34,  5.89it/s][A
Batch #:  16%|â–ˆâ–Œ        | 37/237 [00:07<00:35,  5.61it/s][A
Batch #:  16%|â–ˆâ–Œ        | 38/237 [00:07<00:33,  5.99it/s][A
Batch #:  16%|â–ˆâ–‹        | 39/237 [00:07<00:32,  6.07it/s][A
Batch #:  17%|â–ˆâ–‹        | 40/237 [00:08<01:03,  3.11it/s][A
Batch #:  17%|â–ˆâ–‹        | 41/237 [00:08<00:53,  3.65it/s][A
Batch #:  18%|â–ˆâ–Š        | 42/237 [00:08<00:45,  4.25it/s][A
Batch #:  18%|â–ˆâ–Š        | 43/237 [00:08<00:42,  4.61it/s][A
Batch #:  19%|â–ˆâ–Š        | 44/237 [00:09<00:39,  4.94it/s][A
Batch #:  19%|â–ˆâ–‰        | 45/237 [00:09<00:36,  5.23it/s][A
Batch #:  19%|â–ˆâ–‰        | 46/237 [00:09<00:34,  5.50it/s][A
Batch #:  20%|â–ˆâ–‰        | 47/237 [00:09<00:34,  5.53it/s][A
Batch #:  20%|â–ˆâ–ˆ        | 48/237 [00:09<00:34,  5.50it/s][A
Batch #:  21%|â–ˆâ–ˆ        | 49/237 [00:09<00:32,  5.76it/s][A
Batch #:  21%|â–ˆâ–ˆ        | 50/237 [00:10<00:51,  3.66it/s][A
Batch #:  22%|â–ˆâ–ˆâ–       | 51/237 [00:10<00:44,  4.19it/s][A
Batch #:  22%|â–ˆâ–ˆâ–       | 52/237 [00:10<00:39,  4.66it/s][A
Batch #:  22%|â–ˆâ–ˆâ–       | 53/237 [00:10<00:36,  5.08it/s][A
Batch #:  23%|â–ˆâ–ˆâ–Ž       | 54/237 [00:11<00:32,  5.57it/s][A
Batch #:  23%|â–ˆâ–ˆâ–Ž       | 55/237 [00:11<00:30,  5.93it/s][A
Batch #:  24%|â–ˆâ–ˆâ–Ž       | 56/237 [00:11<00:29,  6.12it/s][A
Batch #:  24%|â–ˆâ–ˆâ–       | 57/237 [00:11<00:28,  6.40it/s][A
Batch #:  24%|â–ˆâ–ˆâ–       | 58/237 [00:11<00:27,  6.61it/s][A
Batch #:  25%|â–ˆâ–ˆâ–       | 59/237 [00:11<00:28,  6.29it/s][A
Batch #:  25%|â–ˆâ–ˆâ–Œ       | 60/237 [00:12<00:42,  4.16it/s][A
Batch #:  26%|â–ˆâ–ˆâ–Œ       | 61/237 [00:12<00:37,  4.65it/s][A
Batch #:  26%|â–ˆâ–ˆâ–Œ       | 62/237 [00:12<00:35,  4.86it/s][A
Batch #:  27%|â–ˆâ–ˆâ–‹       | 63/237 [00:12<00:31,  5.44it/s][A
Batch #:  27%|â–ˆâ–ˆâ–‹       | 64/237 [00:12<00:29,  5.78it/s][A
Batch #:  27%|â–ˆâ–ˆâ–‹       | 65/237 [00:13<00:28,  6.10it/s][A
Batch #:  28%|â–ˆâ–ˆâ–Š       | 66/237 [00:13<00:27,  6.17it/s][A
Batch #:  28%|â–ˆâ–ˆâ–Š       | 67/237 [00:13<00:28,  6.06it/s][A
Batch #:  29%|â–ˆâ–ˆâ–Š       | 68/237 [00:13<00:27,  6.14it/s][A
Batch #:  29%|â–ˆâ–ˆâ–‰       | 69/237 [00:13<00:26,  6.30it/s][A
Batch #:  30%|â–ˆâ–ˆâ–‰       | 70/237 [00:14<00:41,  4.05it/s][A
Batch #:  30%|â–ˆâ–ˆâ–‰       | 71/237 [00:14<00:37,  4.40it/s][A
Batch #:  30%|â–ˆâ–ˆâ–ˆ       | 72/237 [00:14<00:33,  4.99it/s][A
Batch #:  31%|â–ˆâ–ˆâ–ˆ       | 73/237 [00:14<00:30,  5.41it/s][A
Batch #:  31%|â–ˆâ–ˆâ–ˆ       | 74/237 [00:14<00:29,  5.60it/s][A
Batch #:  32%|â–ˆâ–ˆâ–ˆâ–      | 75/237 [00:14<00:27,  5.95it/s][A
Batch #:  32%|â–ˆâ–ˆâ–ˆâ–      | 76/237 [00:15<00:26,  6.17it/s][A
Batch #:  32%|â–ˆâ–ˆâ–ˆâ–      | 77/237 [00:15<00:25,  6.33it/s][A
Batch #:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 78/237 [00:15<00:25,  6.20it/s][A
Batch #:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 79/237 [00:15<00:24,  6.36it/s][A
Batch #:  34%|â–ˆâ–ˆâ–ˆâ–      | 80/237 [00:15<00:37,  4.20it/s][A
Batch #:  34%|â–ˆâ–ˆâ–ˆâ–      | 81/237 [00:16<00:35,  4.34it/s][A
Batch #:  35%|â–ˆâ–ˆâ–ˆâ–      | 82/237 [00:16<00:32,  4.83it/s][A
Batch #:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 83/237 [00:16<00:32,  4.67it/s][A
Batch #:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 84/237 [00:16<00:33,  4.60it/s][A
Batch #:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/237 [00:16<00:30,  5.05it/s][A
Batch #:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 86/237 [00:17<00:29,  5.14it/s][A
Batch #:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 87/237 [00:17<00:27,  5.42it/s][A
Batch #:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 88/237 [00:17<00:27,  5.34it/s][A
Batch #:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 89/237 [00:17<00:26,  5.62it/s][A
Batch #:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 90/237 [00:18<00:48,  3.04it/s][A
Batch #:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 91/237 [00:18<00:41,  3.52it/s][A
Batch #:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 92/237 [00:18<00:34,  4.14it/s][A
Batch #:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 93/237 [00:18<00:31,  4.62it/s][A
Batch #:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 94/237 [00:18<00:27,  5.24it/s][A
Batch #:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 95/237 [00:19<00:24,  5.78it/s][A
Batch #:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 96/237 [00:19<00:22,  6.26it/s][A
Batch #:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 97/237 [00:19<00:22,  6.31it/s][A
Batch #:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 98/237 [00:19<00:22,  6.18it/s][A
Batch #:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/237 [00:19<00:22,  6.18it/s][A
Batch #:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/237 [00:20<00:40,  3.42it/s][A
Batch #:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 101/237 [00:20<00:33,  4.02it/s][A
Batch #:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 102/237 [00:20<00:28,  4.66it/s][A
Batch #:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/237 [00:20<00:37,  3.58it/s][A
Batch #:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/237 [00:21<00:32,  4.10it/s][A
Batch #:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/237 [00:21<00:28,  4.67it/s][A
Batch #:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/237 [00:21<00:25,  5.06it/s][A
Batch #:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 107/237 [00:21<00:23,  5.58it/s][A
Batch #:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 108/237 [00:21<00:21,  6.03it/s][A
Batch #:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 109/237 [00:21<00:20,  6.25it/s][A
Batch #:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 110/237 [00:22<00:34,  3.65it/s][A
Batch #:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 111/237 [00:22<00:30,  4.16it/s][A
Batch #:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 112/237 [00:22<00:25,  4.81it/s][A
Batch #:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 113/237 [00:22<00:24,  5.15it/s][A
Batch #:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 114/237 [00:22<00:22,  5.55it/s][A
Batch #:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/237 [00:23<00:20,  5.90it/s][A
Batch #:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 116/237 [00:23<00:23,  5.26it/s][A
Batch #:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 117/237 [00:23<00:21,  5.70it/s][A
Batch #:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 118/237 [00:23<00:20,  5.93it/s][A
Batch #:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 119/237 [00:23<00:22,  5.24it/s][A
Batch #:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 120/237 [00:24<00:33,  3.54it/s][A
Batch #:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/237 [00:24<00:27,  4.17it/s][A
Batch #:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 122/237 [00:24<00:23,  4.82it/s][A
Batch #:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 123/237 [00:24<00:21,  5.31it/s][A
Batch #:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 124/237 [00:25<00:24,  4.64it/s][A
Batch #:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 125/237 [00:25<00:22,  5.02it/s][A
Batch #:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 126/237 [00:25<00:20,  5.46it/s][A
Batch #:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/237 [00:25<00:18,  5.93it/s][A
Batch #:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/237 [00:25<00:17,  6.35it/s][A
Batch #:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/237 [00:25<00:17,  6.35it/s][A
Batch #:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/237 [00:26<00:26,  4.08it/s][A
Batch #:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 131/237 [00:26<00:25,  4.15it/s][A
Batch #:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 132/237 [00:26<00:21,  4.81it/s][A
Batch #:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 133/237 [00:26<00:19,  5.31it/s][A
Batch #:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/237 [00:26<00:17,  5.80it/s][A
Batch #:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 135/237 [00:27<00:17,  5.93it/s][A
Batch #:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 136/237 [00:27<00:16,  6.07it/s][A
Batch #:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 137/237 [00:27<00:16,  6.08it/s][A
Batch #:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 138/237 [00:27<00:16,  6.07it/s][A
Batch #:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 139/237 [00:27<00:17,  5.65it/s][A
Batch #:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 140/237 [00:28<00:25,  3.85it/s][A
Batch #:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 141/237 [00:28<00:22,  4.34it/s][A
Batch #:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 142/237 [00:28<00:19,  4.96it/s][A
Batch #:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 143/237 [00:28<00:17,  5.47it/s][A
Batch #:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 144/237 [00:28<00:18,  5.00it/s][A
Batch #:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 145/237 [00:29<00:17,  5.15it/s][A
Batch #:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 146/237 [00:29<00:16,  5.66it/s][A
Batch #:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 147/237 [00:29<00:15,  5.88it/s][A
Batch #:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/237 [00:29<00:14,  6.33it/s][A
Batch #:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 149/237 [00:29<00:13,  6.54it/s][A
Batch #:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/237 [00:30<00:21,  4.01it/s][A
Batch #:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 151/237 [00:30<00:19,  4.49it/s][A
Batch #:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 152/237 [00:30<00:19,  4.34it/s][A
Batch #:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 153/237 [00:30<00:17,  4.77it/s][A
Batch #:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 154/237 [00:30<00:16,  5.08it/s][A
Batch #:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 155/237 [00:31<00:14,  5.55it/s][A
Batch #:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 156/237 [00:31<00:13,  6.02it/s][A
Batch #:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 157/237 [00:31<00:12,  6.18it/s][A
Batch #:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 158/237 [00:31<00:12,  6.29it/s][A
Batch #:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 159/237 [00:31<00:11,  6.51it/s][A
Batch #:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 160/237 [00:32<00:18,  4.08it/s][A
Batch #:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 161/237 [00:32<00:16,  4.68it/s][A
Batch #:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 162/237 [00:32<00:14,  5.28it/s][A
Batch #:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 163/237 [00:32<00:12,  5.72it/s][A
Batch #:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 164/237 [00:32<00:12,  6.04it/s][A
Batch #:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/237 [00:32<00:11,  6.40it/s][A
Batch #:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 166/237 [00:32<00:10,  6.60it/s][A
Batch #:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 167/237 [00:33<00:10,  6.88it/s][A
Batch #:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 168/237 [00:33<00:09,  7.01it/s][A
Batch #:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 169/237 [00:33<00:10,  6.68it/s][A
Batch #:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 170/237 [00:33<00:15,  4.29it/s][A
Batch #:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/237 [00:33<00:13,  4.83it/s][A
Batch #:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 172/237 [00:34<00:11,  5.47it/s][A
Batch #:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 173/237 [00:34<00:11,  5.77it/s][A
Batch #:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 174/237 [00:34<00:10,  6.28it/s][A
Batch #:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 175/237 [00:34<00:10,  6.14it/s][A
Batch #:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 176/237 [00:34<00:09,  6.43it/s][A
Batch #:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 177/237 [00:34<00:08,  6.68it/s][A
Batch #:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 178/237 [00:34<00:08,  6.59it/s][A
Batch #:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 179/237 [00:35<00:08,  6.74it/s][A
Batch #:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 180/237 [00:35<00:13,  4.38it/s][A
Batch #:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 181/237 [00:35<00:12,  4.66it/s][A
Batch #:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 182/237 [00:35<00:10,  5.10it/s][A
Batch #:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 183/237 [00:35<00:09,  5.59it/s][A
Batch #:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 184/237 [00:36<00:08,  6.00it/s][A
Batch #:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 185/237 [00:36<00:08,  6.34it/s][A
Batch #:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 186/237 [00:36<00:08,  5.89it/s][A
Batch #:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 187/237 [00:36<00:08,  5.76it/s][A
Batch #:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 188/237 [00:36<00:07,  6.15it/s][A
Batch #:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 189/237 [00:36<00:07,  6.38it/s][A
Batch #:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 190/237 [00:37<00:11,  4.20it/s][AAll seeds set to 42 for random, numpy, torch
2025-05-14 03:38:42,295 - PyTorch version 2.4.1+cu121 available.
2025-05-14 03:38:42,295 - Duckdb version 1.1.3 available.
2025-05-14 03:38:42,926 - Device <cuda:0> is selected
2025-05-14 03:38:42,926 - Device <cuda:0> is selected
2025-05-14 03:38:42,926 - Device <cuda:0> is selected
2025-05-14 03:38:44,283 - Device <cuda:0> is selected

=== Starting task: img_cls_imagenet ===

-- PEFT combo #0: ('low-rank', 'quant-dynamic', 'pruning', 'quant-static') --
2025-05-14 03:38:44,379 - Device <cuda:0> is selected
2025-05-14 03:38:44,808 - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2025-05-14 03:38:44,840 - State start
2025-05-14 03:38:44,842 - Found stale lock file and directory '/tmp/dask-scratch-space/worker-a0xanvxa', purging
2025-05-14 03:38:44,844 - Found stale lock file and directory '/tmp/dask-scratch-space/scheduler-4ani3xh_', purging
2025-05-14 03:38:44,850 -   Scheduler at: inproc://10.32.15.89/292021/1
2025-05-14 03:38:44,850 -   dashboard at:  http://10.32.15.89:39519/status
2025-05-14 03:38:44,858 -       Start worker at: inproc://10.32.15.89/292021/4
2025-05-14 03:38:44,858 -          Listening to:          inproc10.32.15.89
2025-05-14 03:38:44,858 -           Worker name:                          0
2025-05-14 03:38:44,859 -          dashboard at:          10.32.15.89:40403
2025-05-14 03:38:44,859 - Waiting to connect to: inproc://10.32.15.89/292021/1
2025-05-14 03:38:44,859 - -------------------------------------------------
2025-05-14 03:38:44,859 -               Threads:                          4
2025-05-14 03:38:44,859 -                Memory:                  62.97 GiB
2025-05-14 03:38:44,859 -       Local Directory: /tmp/dask-scratch-space/worker-qwpbublz
2025-05-14 03:38:44,859 - -------------------------------------------------
2025-05-14 03:38:44,862 - Register worker <WorkerState 'inproc://10.32.15.89/292021/4', name: 0, status: init, memory: 0, processing: 0>
2025-05-14 03:38:44,862 - Starting worker compute stream, inproc://10.32.15.89/292021/4
2025-05-14 03:38:44,862 - Starting established connection to inproc://10.32.15.89/292021/5
2025-05-14 03:38:44,863 -         Registered to: inproc://10.32.15.89/292021/1
2025-05-14 03:38:44,863 - -------------------------------------------------
2025-05-14 03:38:44,863 - Starting established connection to inproc://10.32.15.89/292021/1
2025-05-14 03:38:44,865 - Receive client connection: Client-caa7d4e9-305b-11f0-b4b5-a036bccd403d
2025-05-14 03:38:44,866 - Starting established connection to inproc://10.32.15.89/292021/6
2025-05-14 03:38:44,988 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:38:44,997 - ApiComposer - Pipeline composition started.
2025-05-14 03:40:23,291 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:41:58,586 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:41:58,590 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 03:41:58,833 - ApiComposer - Model generation finished
2025-05-14 03:42:44,841 - FEDOT logger - Final pipeline was fitted
2025-05-14 03:42:44,842 - FEDOT logger - Final pipeline: {'depth': 1, 'length': 1, 'nodes': [low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 03:44:15,182 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:45:40,775 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:45:40,781 - ApiComposer - Pipeline composition started.
2025-05-14 03:46:28,719 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:47:16,988 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:47:16,992 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 03:47:17,242 - ApiComposer - Model generation finished
2025-05-14 03:47:17,841 - FEDOT logger - Final pipeline was fitted
2025-05-14 03:47:17,842 - FEDOT logger - Final pipeline: {'depth': 1, 'length': 1, 'nodes': [quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 03:50:17,647 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:50:17,654 - ApiComposer - Pipeline composition started.
2025-05-14 03:51:11,018 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:51:45,695 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:53:13,177 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:53:13,181 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 03:53:13,437 - ApiComposer - Model generation finished
2025-05-14 03:53:54,176 - FEDOT logger - Final pipeline was fitted
2025-05-14 03:53:54,177 - FEDOT logger - Final pipeline: {'depth': 1, 'length': 1, 'nodes': [pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 03:55:47,626 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 03:56:48,047 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 03:56:48,055 - ApiComposer - Pipeline composition started.
2025-05-14 03:58:14,195 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:59:38,918 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 03:59:38,922 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 03:59:39,364 - ApiComposer - Model generation finished
2025-05-14 03:59:56,987 - FEDOT logger - Final pipeline was fitted
2025-05-14 03:59:56,988 - FEDOT logger - Final pipeline: {'depth': 2, 'length': 2, 'nodes': [quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 04:02:18,030 - Failed to reconnect to scheduler after 30.00 seconds, closing client

-- PEFT combo #1: ('low-rank', 'quant-dynamic', 'pruning', 'quant-qat') --
2025-05-14 04:04:30,026 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:04:30,034 - ApiComposer - Pipeline composition started.
2025-05-14 04:07:05,921 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:09:42,926 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:09:42,931 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:09:43,369 - ApiComposer - Model generation finished
2025-05-14 04:10:00,016 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:11:16,765 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:11:16,766 - FEDOT logger - Final pipeline: {'depth': 2, 'length': 2, 'nodes': [low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 04:16:03,974 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:16:03,986 - ApiComposer - Pipeline composition started.
2025-05-14 04:17:49,979 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:19:42,489 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:19:42,494 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:19:42,907 - ApiComposer - Model generation finished
2025-05-14 04:20:00,059 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:20:00,059 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 3, 'nodes': [quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 04:21:34,012 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:25:50,601 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:25:50,609 - ApiComposer - Pipeline composition started.
2025-05-14 04:28:19,824 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:30:47,724 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:30:47,728 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:30:48,012 - ApiComposer - Model generation finished
2025-05-14 04:31:20,582 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:32:06,817 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:32:06,818 - FEDOT logger - Final pipeline: {'depth': 2, 'length': 2, 'nodes': [pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 04:36:31,877 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:36:31,888 - ApiComposer - Pipeline composition started.
2025-05-14 04:39:14,528 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:41:56,605 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:41:56,609 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:41:56,897 - ApiComposer - Model generation finished
2025-05-14 04:42:01,859 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:42:50,849 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:42:50,849 - FEDOT logger - Final pipeline: {'depth': 4, 'length': 4, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #2: ('low-rank', 'quant-static', 'pruning', 'quant-dynamic') --
2025-05-14 04:50:03,379 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 04:50:03,388 - ApiComposer - Pipeline composition started.
2025-05-14 04:53:45,759 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:55:33,367 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 04:57:23,719 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 04:57:23,725 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 04:57:24,006 - ApiComposer - Model generation finished
2025-05-14 04:59:35,767 - FEDOT logger - Final pipeline was fitted
2025-05-14 04:59:35,767 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 3, 'nodes': [low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 05:05:28,510 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 05:05:28,524 - ApiComposer - Pipeline composition started.
2025-05-14 05:08:55,356 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:10:58,493 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 05:12:12,075 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:12:12,080 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 05:12:12,381 - ApiComposer - Model generation finished
2025-05-14 05:13:22,147 - FEDOT logger - Final pipeline was fitted
2025-05-14 05:13:22,148 - FEDOT logger - Final pipeline: {'depth': 5, 'length': 5, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 05:22:20,909 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 05:22:20,918 - ApiComposer - Pipeline composition started.
2025-05-14 05:25:44,926 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:27:50,884 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 05:29:10,728 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:29:10,733 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 05:29:11,024 - ApiComposer - Model generation finished
2025-05-14 05:31:10,099 - FEDOT logger - Final pipeline was fitted
2025-05-14 05:31:10,100 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 3, 'nodes': [pruning_model, pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 05:37:02,866 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 05:37:02,879 - ApiComposer - Pipeline composition started.
2025-05-14 05:40:40,832 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:42:32,844 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 05:44:13,744 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 05:44:13,749 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 05:44:14,043 - ApiComposer - Model generation finished
2025-05-14 05:45:24,685 - FEDOT logger - Final pipeline was fitted
2025-05-14 05:45:24,685 - FEDOT logger - Final pipeline: {'depth': 6, 'length': 6, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #3: ('low-rank', 'quant-static', 'pruning', 'quant-qat') --
2025-05-14 05:55:58,309 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 05:55:58,320 - ApiComposer - Pipeline composition started.
2025-05-14 06:00:49,986 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:01:28,298 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 06:05:29,608 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:05:29,614 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 06:05:29,905 - ApiComposer - Model generation finished
2025-05-14 06:08:25,009 - FEDOT logger - Final pipeline was fitted
2025-05-14 06:08:25,010 - FEDOT logger - Final pipeline: {'depth': 4, 'length': 4, 'nodes': [low_rank_model, low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 06:15:48,276 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 06:15:48,291 - ApiComposer - Pipeline composition started.
2025-05-14 06:20:07,358 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:21:18,258 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 06:24:15,358 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:24:15,364 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 06:24:15,686 - ApiComposer - Model generation finished
2025-05-14 06:25:42,635 - FEDOT logger - Final pipeline was fitted
2025-05-14 06:25:42,636 - FEDOT logger - Final pipeline: {'depth': 7, 'length': 7, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 06:37:27,048 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 06:37:27,059 - ApiComposer - Pipeline composition started.
2025-05-14 06:41:51,222 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:42:57,026 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 06:46:09,164 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 06:46:09,168 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 06:46:09,472 - ApiComposer - Model generation finished
2025-05-14 06:48:45,928 - FEDOT logger - Final pipeline was fitted
2025-05-14 06:48:45,928 - FEDOT logger - Final pipeline: {'depth': 4, 'length': 4, 'nodes': [pruning_model, pruning_model, pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 06:56:05,753 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 06:56:05,770 - ApiComposer - Pipeline composition started.
2025-05-14 07:01:09,399 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:01:35,729 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 07:06:03,917 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:06:03,924 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 07:06:04,255 - ApiComposer - Model generation finished
2025-05-14 07:08:01,112 - FEDOT logger - Final pipeline was fitted
2025-05-14 07:08:01,114 - FEDOT logger - Final pipeline: {'depth': 8, 'length': 8, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #4: ('low-rank', 'quant-qat', 'pruning', 'quant-dynamic') --
2025-05-14 07:20:54,845 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 07:20:54,857 - ApiComposer - Pipeline composition started.
2025-05-14 07:26:24,833 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 07:26:44,558 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:32:26,842 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:32:26,849 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 07:32:27,154 - ApiComposer - Model generation finished
2025-05-14 07:36:01,442 - FEDOT logger - Final pipeline was fitted
2025-05-14 07:36:01,442 - FEDOT logger - Final pipeline: {'depth': 5, 'length': 5, 'nodes': [low_rank_model, low_rank_model, low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 07:45:31,370 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 07:45:31,390 - ApiComposer - Pipeline composition started.
2025-05-14 07:51:01,583 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 07:51:33,644 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:57:29,886 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 07:57:29,893 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 07:57:30,255 - ApiComposer - Model generation finished
2025-05-14 08:00:08,136 - FEDOT logger - Final pipeline was fitted
2025-05-14 08:00:08,137 - FEDOT logger - Final pipeline: {'depth': 9, 'length': 9, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 08:14:43,758 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 08:14:43,771 - ApiComposer - Pipeline composition started.
2025-05-14 08:19:59,327 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:20:13,736 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 08:25:36,935 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:25:36,942 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 08:25:37,478 - ApiComposer - Model generation finished
2025-05-14 08:29:20,522 - FEDOT logger - Final pipeline was fitted
2025-05-14 08:29:20,523 - FEDOT logger - Final pipeline: {'depth': 5, 'length': 5, 'nodes': [pruning_model, pruning_model, pruning_model, pruning_model, pruning_model]}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
pruning_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'importance': 'magnitude', 'importance_norm': 1, 'pruning_ratio': 0.5, 'importance_reduction': 'max', 'importance_normalize': 'max', 'pruning_iterations': 1, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 08:37:55,626 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 08:37:55,648 - ApiComposer - Pipeline composition started.
2025-05-14 08:43:25,600 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 08:44:07,838 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:50:06,763 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 08:50:06,771 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 08:50:07,124 - ApiComposer - Model generation finished
2025-05-14 08:52:34,826 - FEDOT logger - Final pipeline was fitted
2025-05-14 08:52:34,827 - FEDOT logger - Final pipeline: {'depth': 10, 'length': 10, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}

-- PEFT combo #5: ('low-rank', 'quant-qat', 'pruning', 'quant-static') --
2025-05-14 09:08:18,541 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 09:08:18,555 - ApiComposer - Pipeline composition started.
2025-05-14 09:13:48,593 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 09:14:59,602 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 09:21:35,993 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 09:21:36,001 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 09:21:36,324 - ApiComposer - Model generation finished
2025-05-14 09:25:50,160 - FEDOT logger - Final pipeline was fitted
2025-05-14 09:25:50,160 - FEDOT logger - Final pipeline: {'depth': 6, 'length': 6, 'nodes': [low_rank_model, low_rank_model, low_rank_model, low_rank_model, low_rank_model, low_rank_model]}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
low_rank_model - {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {'hoer': 5}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'strategy': 'explained_variance', 'rank_prune_each': 1, 'compose_mode': 'two_layers', 'non_adaptive_threshold': 0.8, 'finetune_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 09:35:54,627 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 09:35:54,649 - ApiComposer - Pipeline composition started.
2025-05-14 09:41:24,608 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 09:42:59,556 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 09:50:01,460 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
2025-05-14 09:50:01,468 - GroupedCondition - Optimisation stopped: Time limit is reached
2025-05-14 09:50:01,858 - ApiComposer - Model generation finished
2025-05-14 09:53:13,215 - FEDOT logger - Final pipeline was fitted
2025-05-14 09:53:13,216 - FEDOT logger - Final pipeline: {'depth': 11, 'length': 11, 'nodes': [quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model, quantization_model]}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'qat', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'static', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
quantization_model - {'log_each': None, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}, 'quant_type': 'dynamic', 'allow_emb': False, 'allow_conv': True, 'qat_params': {'log_each': 1, 'eval_each': None, 'save_each': None, 'epochs': 1, 'optimizer': 'adam', 'scheduler': None, 'criterion': 'cross_entropy', 'custom_learning_params': {}, 'custom_criterions': {}, 'model_architecture': {'input_dim': None, 'output_dim': None, 'depth': 3, 'custom_model_params': {}}}}
2025-05-14 10:10:37,464 - ApiComposer - AutoML configured. Parameters tuning: False. Time limit: 0.5 min. Set of candidate models: ['training_model'].
2025-05-14 10:10:37,480 - ApiComposer - Pipeline composition started.
2025-05-14 10:16:07,440 - Failed to reconnect to scheduler after 30.00 seconds, closing client
2025-05-14 10:16:54,788 - FedcoreDispatcher - 1 individuals out of 1 in previous population were evaluated successfully.
