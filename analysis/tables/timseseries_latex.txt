
\begin{table*}[h]
\centering
\caption{Top 5 PEFT Pipelines}
\label{tab:peft_pipelines}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Pipeline} & \textbf{rmse} & \textbf{CPU Latency (ms)} & \textbf{GPU Latency (ms)} & \textbf{CPU Throughput (IPS)} & \textbf{GPU Throughput (IPS)} & \textbf{Model Size (MB)} \
\midrule
pruning_quant-static_low-rank_quant-dynamic & GPU & 3.811 / +10.8\% & $\infty$ & 3.852 / +30.4\% & $\infty$ & 3 / -11.3\% & 0.923 / -68.8\% \\
\cmidrule{1-7}
pruning_quant-static_low-rank_quant-qat & GPU & 3.817 / +10.9\% & $\infty$ & 3.538 / +15.8\% & $\infty$ & 3 / -16.1\% & 0.923 / -68.8\% \\
pruning_quant-qat_low-rank_quant-dynamic & GPU & 3.841 / +11.6\% & $\infty$ & 3.664 / +22.8\% & $\infty$ & 3 / -16.1\% & 0.923 / -68.8\% \\
pruning_quant-qat_low-rank_quant-static & CPU & 7.611 / +123.1\% & 0.006 / -7.5\% & $\infty$ & 247 / -21.4\% & $\infty$ & 1.320 / -55.3\% \\
quant-qat_pruning_low-rank_quant-dynamic & CPU & 8.002 / +134.5\% & 0.006 / -1.9\% & $\infty$ & 322 / -4.6\% & $\infty$ & 0.437 / -85.2\% \\
pruning_quant-dynamic_low-rank_quant-qat & CPU & 8.444 / +147.5\% & 0.007 / +22.7\% & $\infty$ & 257 / -28.7\% & $\infty$ & 1.694 / -42.7\% \\
\bottomrule
\end{tabular}
}
\footnotesize
\vspace{0.2cm}
\emph{Note}: Values show final metric / percentage change from original. Best values are bold for non-quantized models and underlined for quantized models. Abbreviations: LR - Low-Rank Decomposition, Pr - Pruning, QAT - Quant-Aware Training, PDQ - Post-training Dynamic Quantization.
\end{table*}
