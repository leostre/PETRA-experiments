
\begin{table*}[h]
\centering
\caption{Top 5 PEFT Pipelines}
\label{tab:peft_pipelines}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Pipeline} & \textbf{f1} & \textbf{CPU Latency (ms)} & \textbf{GPU Latency (ms)} & \textbf{CPU Throughput (IPS)} & \textbf{GPU Throughput (IPS)} & \textbf{Model Size (MB)} \
\midrule
low-rank_quant-qat_pruning_quant-static & GPU & 0.632 / +35.3\% & $\infty$ & 1.995 / +29.4\% & $\infty$ & 18 / +64.6\% & 9.960 / -76.6\% \\
\cmidrule{1-7}
quant-dynamic_pruning_quant-static_low-rank & GPU & 0.500 / +2172.7\% & $\infty$ & 2.306 / +47.8\% & $\infty$ & 16 / +44.1\% & 10.453 / -75.5\% \\
low-rank_quant-qat_pruning_quant-dynamic & GPU & 0.498 / +6.6\% & $\infty$ & 1.948 / +37.9\% & $\infty$ & 18 / +61.5\% & 9.960 / -76.6\% \\
low-rank_quant-qat_pruning_quant-static & CPU & 0.732 / +50.0\% & 0.014 / +2.1\% & $\infty$ & 208 / +35.8\% & $\infty$ & 40.852 / -4.2\% \\
low-rank_quant-qat_pruning_quant-dynamic & CPU & 0.709 / +45.3\% & 0.014 / +1.7\% & $\infty$ & 214 / +35.0\% & $\infty$ & 40.852 / -4.2\% \\
low-rank_quant-dynamic_pruning_quant-qat & CPU & 0.704 / +44.3\% & 0.013 / +4.3\% & $\infty$ & 226 / +38.3\% & $\infty$ & 40.852 / -4.2\% \\
\bottomrule
\end{tabular}
}
\footnotesize
\vspace{0.2cm}
\emph{Note}: Values show final metric / percentage change from original. Best values are bold for non-quantized models and underlined for quantized models. Abbreviations: LR - Low-Rank Decomposition, Pr - Pruning, QAT - Quant-Aware Training, PDQ - Post-training Dynamic Quantization.
\end{table*}
