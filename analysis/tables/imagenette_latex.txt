
\begin{table*}[h]
\centering
\caption{Top 5 PEFT Pipelines}
\label{tab:peft_pipelines}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Pipeline} & \textbf{f1} & \textbf{CPU Latency (ms)} & \textbf{GPU Latency (ms)} & \textbf{CPU Throughput (IPS)} & \textbf{GPU Throughput (IPS)} & \textbf{Model Size (MB)} \
\midrule
pruning_quant-static_low-rank_quant-qat & GPU & 0.477 / +2068.2\% & $\infty$ & 2.057 / +49.9\% & $\infty$ & 17 / +68.3\% & 10.453 / -75.5\% \\
\cmidrule{1-7}
pruning_quant-dynamic_low-rank_quant-qat & GPU & 0.472 / +2045.5\% & $\infty$ & 1.991 / +11.5\% & $\infty$ & 18 / +93.9\% & 10.453 / -75.5\% \\
pruning_quant-static_low-rank_quant-dynamic & GPU & 0.467 / +2022.7\% & $\infty$ & 2.004 / +38.3\% & $\infty$ & 17 / +57.6\% & 10.453 / -75.5\% \\
pruning_quant-dynamic_low-rank_quant-static & GPU & 0.462 / +2000.0\% & $\infty$ & 2.016 / +39.0\% & $\infty$ & 17 / +68.2\% & 10.453 / -75.5\% \\
pruning_quant-qat_low-rank_quant-dynamic & GPU & 0.461 / +1995.5\% & $\infty$ & 2.007 / +43.9\% & $\infty$ & 17 / +60.6\% & 10.453 / -75.5\% \\
\bottomrule
\end{tabular}
}
\footnotesize
\vspace{0.2cm}
\emph{Note}: Values show final metric / percentage change from original. Best values are bold for non-quantized models and underlined for quantized models. Abbreviations: LR - Low-Rank Decomposition, Pr - Pruning, QAT - Quant-Aware Training, PDQ - Post-training Dynamic Quantization.
\end{table*}
