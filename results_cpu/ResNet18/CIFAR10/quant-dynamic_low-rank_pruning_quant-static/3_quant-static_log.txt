
>>> PEFT step 4: quant-static
Creating Dask Server
Generations:   0%|                                                                                                                                                            | 0/10000 [00:00<?, ?gen/s][QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.quint8, quant_min=0, quant_max=255, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: fbgemm, device: cpu
[INIT] dtype: torch.qint8, allow_embedding: False, allow_convolution: True
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.quint8, quant_min=0, quant_max=255, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})}
[MODEL] Model initialized and copied for quantization.
[ParentalReassembler] Residual block 'layer1.base.0.module.module' wrapped with ResidualAddWrapper.
[ParentalReassembler] Residual block 'layer1.base.1.module.module' wrapped with ResidualAddWrapper.
[ParentalReassembler] Residual block 'layer2.0.module.module' wrapped with ResidualAddWrapper.
[PREPARE ERROR] Exception during preparation:
Traceback (most recent call last):
  File "D:\Python\Fedcore\fedcore\fedcore\algorithm\quantization\quantizers.py", line 160, in _prepare_model
    self.quant_model = ParentalReassembler.reassemble(self.quant_model)
  File "D:\Python\Fedcore\fedcore\fedcore\algorithm\quantization\utils.py", line 263, in reassemble
    cls.set_module(model, name, new_module.to(device))
  File "D:\Python\Fedcore\fedcore\fedcore\architecture\abstraction\accessor.py", line 17, in set_module
    parent = reduce(getattr, path, m)
  File "D:\Python\Fedcore\.venv\lib\site-packages\torch\nn\modules\module.py", line 1709, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'ResidualAddWrapper' object has no attribute 'downsample'
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.quint8, quant_min=0, quant_max=255, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: fbgemm, device: cpu
[INIT] dtype: torch.qint8, allow_embedding: False, allow_convolution: True
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.quint8, quant_min=0, quant_max=255, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: fbgemm, device: cpu
[INIT] dtype: torch.qint8, allow_embedding: False, allow_convolution: True
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})}
[MODEL] Model initialized and copied for quantization.
[ParentalReassembler] Residual block 'layer1.base.0.module.module' wrapped with ResidualAddWrapper.
[ParentalReassembler] Residual block 'layer1.base.1.module.module' wrapped with ResidualAddWrapper.
[ParentalReassembler] Residual block 'layer2.0.module.module' wrapped with ResidualAddWrapper.
[PREPARE ERROR] Exception during preparation:
Traceback (most recent call last):
  File "D:\Python\Fedcore\fedcore\fedcore\algorithm\quantization\quantizers.py", line 160, in _prepare_model
    self.quant_model = ParentalReassembler.reassemble(self.quant_model)
  File "D:\Python\Fedcore\fedcore\fedcore\algorithm\quantization\utils.py", line 263, in reassemble
    cls.set_module(model, name, new_module.to(device))
  File "D:\Python\Fedcore\fedcore\fedcore\architecture\abstraction\accessor.py", line 17, in set_module
    parent = reduce(getattr, path, m)
  File "D:\Python\Fedcore\.venv\lib\site-packages\torch\nn\modules\module.py", line 1709, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'ResidualAddWrapper' object has no attribute 'downsample'
Generations:   0%|                                                                                                                                                            | 0/10000 [00:04<?, ?gen/s]
